{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d42660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\n",
      "pandas version: 1.4.3\n",
      "matplotlib version: 3.6.3\n",
      "NumPy version: 1.23.0\n",
      "SciPy version: 1.10.0\n",
      "IPython version: 8.9.0\n",
      "scikit-learn version: 1.2.1\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Python 3.10 Best models Spaceship_Titanic program will you survive on the spaceship titanic or not\n",
    "File name Titanic_eda.py\n",
    "\n",
    "Version: 0.1\n",
    "Author: MLCV\n",
    "Date: 2023-06-25\n",
    "\"\"\"\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ccb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('train.csv')\n",
    "data_val = pd.read_csv('test.csv')\n",
    "\n",
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "print(data_raw.info())\n",
    "\n",
    "data_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e848ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates in train set: {data_raw.duplicated().sum()}, ({np.round(100*data_raw.duplicated().sum()/len(data_raw), 1)}%)')\n",
    "\n",
    "print(f'Duplicates in test set: {data_val.duplicated().sum()}, ({np.round(100*data_val.duplicated().sum()/len(data_val), 1)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ee8a9",
   "metadata": {},
   "source": [
    "### Data Cleaning:\n",
    "\n",
    "First of all, it's necessary to review the data to create an accurate model, after that, it's important to fill in the missing values, trying to obtain a better algorithms performance, so for numerical features will be use the median value, instead of the median or mean and for categorical features will fill using mode and convert to dummy variables.\n",
    "\n",
    "So, the age will be filled with the median, the cockpit attribute will be removed, and landing will be imputed using mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e53f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print('-'*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print('-'*10)\n",
    "\n",
    "data_raw.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d6b99",
   "metadata": {},
   "source": [
    "There are 6 numerical features, 4 categorical features (excluding the target) and 3 descriptive/qualitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80042735",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_feats = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "cat_feats = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "\n",
    "qual_feats = ['PassengerId', 'Cabin', 'Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd28026",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    dataset['Age_group']=np.nan\n",
    "    dataset.loc[dataset['Age']<=12, 'Age_group']='Age_0-12'\n",
    "    dataset.loc[(dataset['Age']>=13) & (dataset['Age']<=17), 'Age_group']='Age_13-17'\n",
    "    dataset.loc[(dataset['Age']>=18) & (dataset['Age']<=25), 'Age_group']='Age_18-25'\n",
    "    dataset.loc[(dataset['Age']>=26) & (dataset['Age']<=30), 'Age_group']='Age_26-30'\n",
    "    dataset.loc[(dataset['Age']>=31) & (dataset['Age']<=50), 'Age_group']='Age_31-50'\n",
    "    dataset.loc[(dataset['Age']>=51), 'Age_group']='Age_51+'\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10,4))\n",
    "g = sns.countplot(data=data1, x='Age_group', hue='Transported', order=['Age_0-12', 'Age_13-17', 'Age_18-25', 'Age_26-30', 'Age_31-50', 'Age_51+'])\n",
    "plt.title('Age group distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    dataset['Expenditure']=dataset[exp_feats].sum(axis=1)\n",
    "    dataset['No_spending']=(dataset['Expenditure']==0).astype(int)\n",
    "    \n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=data1, x='Expenditure', hue='Transported', bins=200)\n",
    "plt.title('Total expenditure (truncated)')\n",
    "plt.ylim([0, 200])\n",
    "plt.xlim([0, 20000])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=data1, x='No_spending', hue='Transported')\n",
    "plt.title('No spending indicator')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff439fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    dataset['Group'] = dataset['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "    dataset['Group_size'] =dataset['Group'].map(lambda x: dataset['Group'].value_counts()[x])\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=data1, x='Group', hue='Transported', binwidth=1)\n",
    "plt.title('Group')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=data1, x='Group_size', hue='Transported')\n",
    "plt.title('Group size')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    dataset['Solo'] =(dataset['Group_size']==1).astype(int)\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(data=data1, x='Solo', hue='Transported')\n",
    "plt.title('Passenger travelling solo or not')\n",
    "plt.ylim([0, 3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca067d0",
   "metadata": {},
   "source": [
    "# Verifying columns probably will drop from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner: \n",
    "    # Replace NaN's with outliers for now (so we can split feature)\n",
    "    dataset['Cabin'].fillna('Z/9999/Z', inplace=True)\n",
    "    # New features\n",
    "    dataset['Cabin_deck'] = dataset['Cabin'].apply(lambda x: x.split('/')[0])\n",
    "    dataset['Cabin_number'] = dataset['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "    dataset['Cabin_side'] = dataset['Cabin'].apply(lambda x: x.split('/')[2])\n",
    "    \n",
    "    # Put Nan's back in (we will fill these later)\n",
    "    dataset.loc[dataset['Cabin_deck']=='Z', 'Cabin_deck']=np.nan\n",
    "    dataset.loc[dataset['Cabin_number']==9999, 'Cabin_number']=np.nan\n",
    "    dataset.loc[dataset['Cabin_side']=='Z', 'Cabin_side']=np.nan\n",
    "    \n",
    "    # Drop Cabin (we don't need it anymore)\n",
    "    dataset.drop('Cabin', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of new features\n",
    "fig=plt.figure(figsize=(10,12))\n",
    "plt.subplot(3,1,1)\n",
    "sns.countplot(data=data1, x='Cabin_deck', hue='Transported', order=['A','B','C','D','E','F','G','T'])\n",
    "plt.title('Cabin deck')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "sns.histplot(data=data1, x='Cabin_number', hue='Transported',binwidth=20)\n",
    "plt.vlines(300, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(600, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(900, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(1200, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(1500, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(1800, ymin=0, ymax=200, color='black')\n",
    "plt.title('Cabin number')\n",
    "plt.xlim([0,2000])\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "sns.countplot(data=data1, x='Cabin_side', hue='Transported')\n",
    "plt.title('Cabin side')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    # New features - training set\n",
    "    dataset['Cabin_region1']=(dataset['Cabin_number']<300).astype(int)   # one-hot encoding\n",
    "    dataset['Cabin_region2']=((dataset['Cabin_number']>=300) & (dataset['Cabin_number']<600)).astype(int)\n",
    "    dataset['Cabin_region3']=((dataset['Cabin_number']>=600) & (dataset['Cabin_number']<900)).astype(int)\n",
    "    dataset['Cabin_region4']=((dataset['Cabin_number']>=900) & (dataset['Cabin_number']<1200)).astype(int)\n",
    "    dataset['Cabin_region5']=((dataset['Cabin_number']>=1200) & (dataset['Cabin_number']<1500)).astype(int)\n",
    "    dataset['Cabin_region6']=((dataset['Cabin_number']>=1500) & (dataset['Cabin_number']<1800)).astype(int)\n",
    "    dataset['Cabin_region7']=(dataset['Cabin_number']>=1800).astype(int)\n",
    "\n",
    "# Plot distribution of new features\n",
    "plt.figure(figsize=(10,4))\n",
    "data1['Cabin_regions_plot']=(data1['Cabin_region1']+2*data1['Cabin_region2']+3*data1['Cabin_region3']+4*data1['Cabin_region4']+5*data1['Cabin_region5']+6*data1['Cabin_region6']+7*data1['Cabin_region7']).astype(int)\n",
    "sns.countplot(data=data1, x='Cabin_regions_plot', hue='Transported')\n",
    "plt.title('Cabin regions')\n",
    "data1.drop('Cabin_regions_plot', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    # Calculate family size from last name.\n",
    "    # Replace NaN's with outliers for now (so we can split feature)\n",
    "    dataset['Name'].fillna('Unknown Unknown', inplace=True)\n",
    "    # New feature - Surname\n",
    "    dataset['Surname']=dataset['Name'].str.split().str[-1]\n",
    "    # New feature - Family size\n",
    "    dataset['Family_size']=dataset['Surname'].map(lambda x: dataset['Surname'].value_counts()[x])\n",
    "    # Put Nan's back in (we will fill these later)\n",
    "    dataset.loc[dataset['Surname']=='Unknown','Surname']=np.nan\n",
    "    dataset.loc[dataset['Family_size']>100,'Family_size']=np.nan\n",
    "    # Drop name (we don't need it anymore)\n",
    "    dataset.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "# New feature distribution\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(data=data1, x='Family_size', hue='Transported')\n",
    "plt.title('Family size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267f3e2",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Transported'].astype(int)\n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    na_cols = dataset.columns[dataset.isna().any()].tolist()\n",
    "    mv=pd.DataFrame(dataset[na_cols].isna().sum(), columns=['Number_missing'])\n",
    "    mv['Percentage_missing']=np.round(100*mv['Number_missing']/len(dataset),2)\n",
    "    print(mv, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['na_count']=data1.isna().sum(axis=1)\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(data=data1, x='na_count', hue='Transported')\n",
    "plt.title('Number of missing entries by passenger')\n",
    "data1.drop('na_count', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8f010",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "\n",
    "In this notebook, the objective is maximise the accurracy of the models, for this we need to look for patterns within the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be783259",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    GHP_gb = dataset.groupby(['Group', 'HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    \n",
    "    HP_bef=dataset['HomePlanet'].isna().sum()\n",
    "    \n",
    "    # Passengers with missing HomePlanet and in a group with known HomePlanet\n",
    "    GHP_index = dataset[dataset['HomePlanet'].isna()][(dataset[dataset['HomePlanet'].isna()]['Group']).isin(GHP_gb.index)].index\n",
    "    \n",
    "    # Fill corresponding missing values\n",
    "    dataset.loc[GHP_index, 'HomePlanet'] = dataset.iloc[GHP_index,:]['Group'].map(lambda x: GHP_gb.idxmax(axis=1)[x])\n",
    "    \n",
    "    # Print number of missing values left\n",
    "    \n",
    "    print('#HomePlanet missing values before:', HP_bef)\n",
    "    print('#HomePlanet missing values after:', dataset['HomePlanet'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notes:\n",
    "\n",
    "Passengers on decks A, B, C or T came from Europa.\n",
    "Passengers on deck G came from Earth.\n",
    "Passengers on decks D, E or F came from multiple planets.\n",
    "'''\n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    # Missing values before\n",
    "    HP_bef = dataset['HomePlanet'].isna().sum()\n",
    "    \n",
    "    # Decks A, B, C OR T came from Europa\n",
    "    dataset.loc[(dataset['HomePlanet'].isna()) & (dataset['Cabin_deck'].isin(['A', 'B', 'C', 'T'])), 'HomePlanet']='Europa'\n",
    "    \n",
    "    # Deck G came from Earth\n",
    "    dataset.loc[(dataset['HomePlanet'].isna()) & (dataset['Cabin_deck']=='G'), 'HomePlanet']='Earth'\n",
    "    \n",
    "    # Print number of missing values left\n",
    "    \n",
    "    print('#HomePlanet missing values before:', HP_bef)\n",
    "    print('#HomePlanet missing values after:', dataset['HomePlanet'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    SHP_gb=data.groupby(['Surname','HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n",
    "    #  Everyone with the same surname comes from the same home planet.\n",
    "    # Missing values before\n",
    "    HP_bef=data['HomePlanet'].isna().sum()\n",
    "\n",
    "    # Passengers with missing HomePlanet and in a family with known HomePlanet\n",
    "    SHP_index=data[data['HomePlanet'].isna()][(data[data['HomePlanet'].isna()]['Surname']).isin(SHP_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[SHP_index,'HomePlanet']=data.iloc[SHP_index,:]['Surname'].map(lambda x: SHP_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#HomePlanet missing values before:',HP_bef)\n",
    "    print('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    HP_bef=data['HomePlanet'].isna().sum()\n",
    "\n",
    "    # Fill remaining HomePlanet missing values with Earth (if not on deck D) or Mars (if on Deck D)\n",
    "    data.loc[(data['HomePlanet'].isna()) & ~(data['Cabin_deck']=='D'), 'HomePlanet']='Earth'\n",
    "    data.loc[(data['HomePlanet'].isna()) & (data['Cabin_deck']=='D'), 'HomePlanet']='Mars'\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#HomePlanet missing values before:',HP_bef)\n",
    "    print('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())\n",
    "\n",
    "# We're done with HomePlanet.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d36e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    D_bef=data['Destination'].isna().sum()\n",
    "\n",
    "    # Fill missing Destination values with mode\n",
    "    data.loc[(data['Destination'].isna()), 'Destination']='TRAPPIST-1e'\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Destination missing values before:',D_bef)\n",
    "    print('#Destination missing values after:',data['Destination'].isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b19df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    # Joint distribution of Group and Surname\n",
    "    GSN_gb = dataset[dataset['Group_size']>1].groupby(['Group','Surname'])['Surname'].size().unstack().fillna(0)\n",
    "    \n",
    "    # The majority (83%) of groups contain only 1 family. So let's fill missing surnames according to the majority surname in that group\n",
    "    # Missing values before\n",
    "    SN_bef = dataset['Surname'].isna().sum()\n",
    "    \n",
    "    # Passengers with missing Surname and in a group with known majority Surname\n",
    "    GSN_index = dataset[dataset['Surname'].isna()][(dataset[dataset['Surname'].isna()]['Group']).isin(GSN_gb.index)].index\n",
    "    \n",
    "    # Fill corresponding missing values\n",
    "    dataset.loc[GSN_index, 'Surname']=dataset.iloc[GSN_index,:]['Group'].map(lambda x: GSN_gb.idxmax(axis=1)[x])\n",
    "    \n",
    "    #Print number of missing values left\n",
    "    print('#Surname missing values before:', SN_bef)\n",
    "    print('#Surname missing values after:', dataset['Surname'].isna().sum())\n",
    "    \n",
    "    # Replace NaN's with ouliers\n",
    "    dataset['Surname'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Update family size feature\n",
    "    dataset['Family_size']=dataset['Surname'].map(lambda x: dataset['Surname'].value_counts()[x])\n",
    "    \n",
    "    # Put NaN's back in place of outliers\n",
    "    dataset.loc[dataset['Surname']=='Unknown', 'Surname']=np.nan\n",
    "    \n",
    "    # Say unknown surname means no family\n",
    "    dataset.loc[dataset['Family_size']>100,'Family_size']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution of Group and Cabin features\n",
    "    GCD_gb=data[data['Group_size']>1].groupby(['Group','Cabin_deck'])['Cabin_deck'].size().unstack().fillna(0)\n",
    "    GCN_gb=data[data['Group_size']>1].groupby(['Group','Cabin_number'])['Cabin_number'].size().unstack().fillna(0)\n",
    "    GCS_gb=data[data['Group_size']>1].groupby(['Group','Cabin_side'])['Cabin_side'].size().unstack().fillna(0)\n",
    "\n",
    "    # Everyone in the same group is also on the same cabin side. For cabin deck and cabin number there is also a fairly good (but not perfect) correlation with group.\n",
    "    # Missing values before\n",
    "    CS_bef=data['Cabin_side'].isna().sum()\n",
    "\n",
    "    # Passengers with missing Cabin side and in a group with known Cabin side\n",
    "    GCS_index=data[data['Cabin_side'].isna()][(data[data['Cabin_side'].isna()]['Group']).isin(GCS_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[GCS_index,'Cabin_side']=data.iloc[GCS_index,:]['Group'].map(lambda x: GCS_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_side missing values before:',CS_bef)\n",
    "    print('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for data in data_cleaner:\n",
    "    # Joint distribution of Surname and Cabin side\n",
    "    SCS_gb=data[data['Group_size']>1].groupby(['Surname','Cabin_side'])['Cabin_side'].size().unstack().fillna(0)\n",
    "\n",
    "    # Ratio of sides\n",
    "    SCS_gb['Ratio']=SCS_gb['P']/(SCS_gb['P']+SCS_gb['S'])\n",
    "\n",
    "    # Histogram of ratio\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.histplot(SCS_gb['Ratio'], kde=True, binwidth=0.05)\n",
    "    plt.title('Ratio of cabin side by surname')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306873fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print proportion\n",
    "print('Percentage of families all on the same cabin side:', 100*np.round((SCS_gb['Ratio'].isin([0,1])).sum()/len(SCS_gb),3),'%')\n",
    "\n",
    "# Another view of the same information\n",
    "SCS_gb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values before\n",
    "CS_bef=data['Cabin_side'].isna().sum()\n",
    "\n",
    "# Drop ratio column\n",
    "SCS_gb.drop('Ratio', axis=1, inplace=True)\n",
    "    \n",
    "for data in data_cleaner:\n",
    "    # Passengers with missing Cabin side and in a family with known Cabin side\n",
    "    SCS_index=data[data['Cabin_side'].isna()][(data[data['Cabin_side'].isna()]['Surname']).isin(SCS_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[SCS_index,'Cabin_side']=data.iloc[SCS_index,:]['Surname'].map(lambda x: SCS_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Drop surname (we don't need it anymore)\n",
    "    data.drop('Surname', axis=1, inplace=True)\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_side missing values before:',CS_bef)\n",
    "    print('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f167ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Value counts\n",
    "    data['Cabin_side'].value_counts()\n",
    "\n",
    "    # Missing values before\n",
    "    CS_bef=data['Cabin_side'].isna().sum()\n",
    "\n",
    "    # Fill remaining missing values with outlier\n",
    "    data.loc[data['Cabin_side'].isna(),'Cabin_side']='Z'\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_side missing values before:',CS_bef)\n",
    "    print('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f94e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    CD_bef=data['Cabin_deck'].isna().sum()\n",
    "\n",
    "    # Passengers with missing Cabin deck and in a group with known majority Cabin deck\n",
    "    GCD_index=data[data['Cabin_deck'].isna()][(data[data['Cabin_deck'].isna()]['Group']).isin(GCD_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[GCD_index,'Cabin_deck']=data.iloc[GCD_index,:]['Group'].map(lambda x: GCD_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_deck missing values before:',CD_bef)\n",
    "    print('#Cabin_deck missing values after:',data['Cabin_deck'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ecfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['HomePlanet', 'Destination', 'Solo', 'Cabin_deck'])['Cabin_deck'].size().unstack().fillna(0)\n",
    "    \n",
    "    '''\n",
    "    Passengers from Mars are most likely in deck F.\n",
    "    Passengers from Europa are (more or less) most likely in deck C if travelling solo and deck B otherwise.\n",
    "    Passengers from Earth are (more or less) most likely in deck G.\n",
    "    We will fill in missing values according to where the mode appears in these subgroups.\n",
    "    '''\n",
    "    \n",
    "    # Missing values before\n",
    "    CD_bef = data['Cabin_deck'].isna().sum()\n",
    "    \n",
    "    # Fill missing values using the mode\n",
    "    na_rows_CD = data.loc[data['Cabin_deck'].isna(),'Cabin_deck'].index\n",
    "    data.loc[data['Cabin_deck'].isna(),'Cabin_deck']=data.groupby(['HomePlanet', 'Destination', 'Solo'])['Cabin_deck'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))[na_rows_CD]\n",
    "    \n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_deck missing values before:',CD_bef)\n",
    "    print('#Cabin_deck missing values after:',data['Cabin_deck'].isna().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02116cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Scatterplot\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.scatterplot(x=data['Cabin_number'], y=data['Group'], c=LabelEncoder().fit_transform(data.loc[~data['Cabin_number'].isna(),'Cabin_deck']), cmap='tab10')\n",
    "    plt.title('Cabin_number vs group coloured by group ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e23c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    CN_bef=data['Cabin_number'].isna().sum()\n",
    "    print('#Cabin_number missing values before:',CN_bef)\n",
    "    # Extrapolate linear relationship on a deck by deck basis\n",
    "    for deck in ['A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        # Features and labels\n",
    "        X_CN=data.loc[~(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Group']\n",
    "        y_CN=data.loc[~(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Cabin_number']\n",
    "        X_test_CN=data.loc[(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Group']\n",
    "\n",
    "        if not X_test_CN.empty:\n",
    "            # Linear regression\n",
    "            model_CN=sklearn.linear_model.LinearRegression()\n",
    "            model_CN.fit(X_CN.values.reshape(-1, 1), y_CN)\n",
    "            preds_CN=model_CN.predict(X_test_CN.values.reshape(-1, 1))\n",
    "\n",
    "            # Fill missing values with predictions\n",
    "            data.loc[(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Cabin_number']=preds_CN.astype(int)\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_number missing values before:',CN_bef)\n",
    "    print('#Cabin_number missing values after:',data['Cabin_number'].isna().sum())\n",
    "\n",
    "    # One-hot encode cabin regions\n",
    "    data['Cabin_region1']=(data['Cabin_number']<300).astype(int)\n",
    "    data['Cabin_region2']=((data['Cabin_number']>=300) & (data['Cabin_number']<600)).astype(int)\n",
    "    data['Cabin_region3']=((data['Cabin_number']>=600) & (data['Cabin_number']<900)).astype(int)\n",
    "    data['Cabin_region4']=((data['Cabin_number']>=900) & (data['Cabin_number']<1200)).astype(int)\n",
    "    data['Cabin_region5']=((data['Cabin_number']>=1200) & (data['Cabin_number']<1500)).astype(int)\n",
    "    data['Cabin_region6']=((data['Cabin_number']>=1500) & (data['Cabin_number']<1800)).astype(int)\n",
    "    data['Cabin_region7']=(data['Cabin_number']>=1800).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242112e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    V_bef=data['VIP'].isna().sum()\n",
    "\n",
    "    # Fill missing values with mode\n",
    "    data.loc[data['VIP'].isna(),'VIP']=False\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#VIP missing values before:',V_bef)\n",
    "    print('#VIP missing values after:',data['VIP'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a171eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['HomePlanet','No_spending','Solo','Cabin_deck'])['Age'].median().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    A_bef=data[exp_feats].isna().sum().sum()\n",
    "\n",
    "    # Fill missing values using the median\n",
    "    na_rows_A=data.loc[data['Age'].isna(),'Age'].index\n",
    "    data.loc[data['Age'].isna(),'Age']=data.groupby(['HomePlanet','No_spending','Solo','Cabin_deck'])['Age'].transform(lambda x: x.fillna(x.median()))[na_rows_A]\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Age missing values before:',A_bef)\n",
    "    print('#Age missing values after:',data['Age'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6bf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Update age group feature\n",
    "    data.loc[data['Age']<=12,'Age_group']='Age_0-12'\n",
    "    data.loc[(data['Age']>12) & (data['Age']<18),'Age_group']='Age_13-17'\n",
    "    data.loc[(data['Age']>=18) & (data['Age']<=25),'Age_group']='Age_18-25'\n",
    "    data.loc[(data['Age']>25) & (data['Age']<=30),'Age_group']='Age_26-30'\n",
    "    data.loc[(data['Age']>30) & (data['Age']<=50),'Age_group']='Age_31-50'\n",
    "    data.loc[data['Age']>50,'Age_group']='Age_51+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ef235",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['No_spending','CryoSleep'])['CryoSleep'].size().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    CSL_bef=data['CryoSleep'].isna().sum()\n",
    "\n",
    "    # Fill missing values using the mode\n",
    "    na_rows_CSL=data.loc[data['CryoSleep'].isna(),'CryoSleep'].index\n",
    "    data.loc[data['CryoSleep'].isna(),'CryoSleep']=data.groupby(['No_spending'])['CryoSleep'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))[na_rows_CSL]\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#CryoSleep missing values before:',CSL_bef)\n",
    "    print('#CryoSleep missing values after:',data['CryoSleep'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    E_bef=data[exp_feats].isna().sum().sum()\n",
    "\n",
    "    # CryoSleep has no expenditure\n",
    "    for col in exp_feats:\n",
    "        data.loc[(data[col].isna()) & (data['CryoSleep']==True), col]=0\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Expenditure missing values before:',E_bef)\n",
    "    print('#Expenditure missing values after:',data[exp_feats].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['HomePlanet','Solo','Age_group'])['Expenditure'].mean().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    E_bef=data[exp_feats].isna().sum().sum()\n",
    "\n",
    "    # Fill remaining missing values using the median\n",
    "    for col in exp_feats:\n",
    "        na_rows=data.loc[data[col].isna(),col].index\n",
    "        data.loc[data[col].isna(),col]=data.groupby(['HomePlanet','Solo','Age_group'])[col].transform(lambda x: x.fillna(x.mean()))[na_rows]\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Expenditure missing values before:',E_bef)\n",
    "    print('#Expenditure missing values after:',data[exp_feats].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992aaf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_cleaner:\n",
    "    # Update expenditure and no_spending\n",
    "    data['Expenditure']=data[exp_feats].sum(axis=1)\n",
    "    data['No_spending']=(data['Expenditure']==0).astype(int)\n",
    "    data.isna().sum()\n",
    "    # Apply log transform\n",
    "    for col in ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','Expenditure']:\n",
    "        data[col]=np.log(1+data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56a7c3",
   "metadata": {},
   "source": [
    "# Convert Formats\n",
    "\n",
    "Convert categorical formats data to dummy variables for mathematical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "\n",
    "for data in data_cleaner:\n",
    "    data['HomePlanet_Code'] = label.fit_transform(data['HomePlanet'])\n",
    "    data['CryoSleep_Code'] = label.fit_transform(data['CryoSleep'])\n",
    "    data['Destination_Code'] = label.fit_transform(data['Destination'])\n",
    "    data['VIP_Code'] = label.fit_transform(data['VIP'])\n",
    "    data['Age_group_Code'] = label.fit_transform(data['Age_group'])\n",
    "    data['Cabin_deck_Code'] = label.fit_transform(data['Cabin_deck'])\n",
    "    data['Cabin_side_Code'] = label.fit_transform(data['Cabin_side'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f199e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y variable aka target/outcome\n",
    "Target = ['Transported']\n",
    "\n",
    "# define x variables for original features aka feature selection\n",
    "\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'\n",
    "data1_x = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] # Original data\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'Age_group', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_deck', 'Cabin_number', 'Cabin_side', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'\n",
    "\n",
    "data1_x_calc = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_number', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'] # coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "# define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Age', 'No_spending', 'Group_size', 'Solo', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "data1_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e730f",
   "metadata": {},
   "source": [
    "# Splitting Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x , test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state=0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target], random_state=0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state=0)\n",
    "\n",
    "print('Data1 Shape: {}'.format(data1.shape))\n",
    "print('Train1 Shape: {}'.format(train1_x.shape))\n",
    "print('Test1 Shape: {}'.format(test1_x.shape))\n",
    "\n",
    "train1_x_bin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754464f",
   "metadata": {},
   "source": [
    "# Perform Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ab9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64':\n",
    "        print('Transported Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ac958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize': 5 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be05920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plots of entire dataset\n",
    "pp = sns.pairplot(data1, hue = 'Transported', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc110283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa83148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: This is a handmade model for learning purposes only.\n",
    "#However, it is possible to create your own predictive model without a fancy algorithm :)\n",
    "\n",
    "#coin flip model with random 1/survived 0/died\n",
    "\n",
    "#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\n",
    "for index, row in data1.iterrows(): \n",
    "    #random number generator: https://docs.python.org/2/library/random.html\n",
    "    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n",
    "        data1.at[index, 'Random_Predict'] = 1 # predict survived/1\n",
    "    else: \n",
    "        data1.at[index, 'Random_Predict'] = 0 # predict died/0\n",
    "    \n",
    "\n",
    "#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n",
    "#the mean of the column will then equal the accuracy\n",
    "data1['Random_Score'] = 0 #assume prediction wrong\n",
    "data1.loc[(data1['Transported'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\n",
    "print('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n",
    "\n",
    "#we can also use scikit's accuracy_score function to save us a few lines of code\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1[Target], data1['Random_Predict'])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6914da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\n",
    "def mytree(df):\n",
    "    \n",
    "    #initialize table to store predictions\n",
    "    Model = pd.DataFrame(data = {'Predict':[]})\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #Question 1: Age_group (55-69%)\n",
    "        if (df.loc[index, 'Age_group'] == 'Age_0-12') or (df.loc[index, 'Age_group'] == 'Age_13-17'):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "                \n",
    "        #Question 2: HomePlanet_Code (66-67%)\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1 \n",
    "        \n",
    "        #Question 3: VIP_Code (71%)\n",
    "        if (df.loc[index, 'VIP_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        \n",
    "        #Question 4: CryoSleep_Code (68-81%)\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1  \n",
    "                \n",
    "        #Question 5: Cabin_deck_Code_Code (73-80%)\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 7):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 4):\n",
    "                  Model.loc[index, 'Predict'] = 0                \n",
    "        \n",
    "        #Question 6: Cabin_side_Code (72%)\n",
    "        if (df.loc[index, 'Cabin_side_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "                \n",
    "        \n",
    "    return Model\n",
    "\n",
    "\n",
    "#model data\n",
    "Tree_Predict = mytree(data1)\n",
    "print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Transported'], Tree_Predict)*100))\n",
    "\n",
    "\n",
    "#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "print(metrics.classification_report(data1['Transported'], Tree_Predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcea720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracy Summary\n",
    "#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(data1['Transported'], Tree_Predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['NotTransported', 'Transported']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Normalized confusion matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#duplicates gridsearchcv\n",
    "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
    "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
    "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
    "#print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41486e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcada58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
    "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
    "correlation_heatmap(MLA_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why choose one model, when you can pick them all with voting classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n",
    "\n",
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: THIS SECTION IS UNDER CONSTRUCTION!!!!\n",
    "#UPDATE: This section was scrapped for the next section; as it's more computational friendly.\n",
    "\n",
    "#WARNING: Running is very computational intensive and time expensive\n",
    "#code is written for experimental/developmental purposes and not production ready\n",
    "\n",
    "\n",
    "#tune each estimator before creating a super model\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [50,100,300]\n",
    "grid_ratio = [.1,.25,.5,.75,1.0]\n",
    "grid_learn = [.01,.03,.05,.1,.25]\n",
    "grid_max_depth = [2,4,6,None]\n",
    "grid_min_samples = [5,10,.03,.05,.10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "vote_param = [{\n",
    "#            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'ada__n_estimators': grid_n_estimator,\n",
    "            'ada__learning_rate': grid_ratio,\n",
    "            'ada__algorithm': ['SAMME', 'SAMME.R'],\n",
    "            'ada__random_state': grid_seed,\n",
    "    \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'bc__n_estimators': grid_n_estimator,\n",
    "            'bc__max_samples': grid_ratio,\n",
    "            'bc__oob_score': grid_bool, \n",
    "            'bc__random_state': grid_seed,\n",
    "            \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'etc__n_estimators': grid_n_estimator,\n",
    "            'etc__criterion': grid_criterion,\n",
    "            'etc__max_depth': grid_max_depth,\n",
    "            'etc__random_state': grid_seed,\n",
    "\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            'gbc__loss': ['deviance', 'exponential'],\n",
    "            'gbc__learning_rate': grid_ratio,\n",
    "            'gbc__n_estimators': grid_n_estimator,\n",
    "            'gbc__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "            'gbc__max_depth': grid_max_depth,\n",
    "            'gbc__min_samples_split': grid_min_samples,\n",
    "            'gbc__min_samples_leaf': grid_min_samples,      \n",
    "            'gbc__random_state': grid_seed,\n",
    "    \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'rfc__n_estimators': grid_n_estimator,\n",
    "            'rfc__criterion': grid_criterion,\n",
    "            'rfc__max_depth': grid_max_depth,\n",
    "            'rfc__min_samples_split': grid_min_samples,\n",
    "            'rfc__min_samples_leaf': grid_min_samples,   \n",
    "            'rfc__bootstrap': grid_bool,\n",
    "            'rfc__oob_score': grid_bool, \n",
    "            'rfc__random_state': grid_seed,\n",
    "        \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'lr__fit_intercept': grid_bool,\n",
    "            'lr__penalty': ['l1','l2'],\n",
    "            'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'lr__random_state': grid_seed,\n",
    "            \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'bnb__alpha': grid_ratio,\n",
    "            'bnb__prior': grid_bool,\n",
    "            'bnb__random_state': grid_seed,\n",
    "    \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'knn__n_neighbors': [1,2,3,4,5,6,7],\n",
    "            'knn__weights': ['uniform', 'distance'],\n",
    "            'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'knn__random_state': grid_seed,\n",
    "            \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'svc__C': grid_max_depth,\n",
    "            'svc__gamma': grid_ratio,\n",
    "            'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "            'svc__probability': [True],\n",
    "            'svc__random_state': grid_seed,\n",
    "    \n",
    "    \n",
    "            #http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'xgb__learning_rate': grid_ratio,\n",
    "            'xgb__max_depth': [2,4,6,8,10],\n",
    "            'xgb__tree_method': ['exact', 'approx', 'hist'],\n",
    "            'xgb__objective': ['reg:linear', 'reg:logistic', 'binary:logistic'],\n",
    "            'xgb__seed': grid_seed  \n",
    "\n",
    "        }]\n",
    "\n",
    "#Soft Vote with tuned models\n",
    "#grid_soft = model_selection.GridSearchCV(estimator = vote_soft, param_grid = vote_param, cv = 2, scoring = 'roc_auc')\n",
    "#grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(grid_soft.cv_results_.keys())\n",
    "#print(grid_soft.cv_results_['params'])\n",
    "#print('Soft Vote Tuned Parameters: ', grid_soft.best_params_)\n",
    "#print(grid_soft.cv_results_['mean_train_score'])\n",
    "#print(\"Soft Vote Tuned Training w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(grid_soft.cv_results_['mean_test_score'])\n",
    "#print(\"Soft Vote Tuned Test w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "#print(\"Soft Vote Tuned Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "#print('-'*10)\n",
    "\n",
    "\n",
    "#credit: https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/\n",
    "#cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "#for r, _ in enumerate(grid_soft.cv_results_['mean_test_score']):\n",
    "#    print(\"%0.3f +/- %0.2f %r\"\n",
    "#          % (grid_soft.cv_results_[cv_keys[0]][r],\n",
    "#             grid_soft.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "#             grid_soft.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "#print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: Running is very computational intensive and time expensive.\n",
    "#Code is written for experimental/developmental purposes and not production ready!\n",
    "\n",
    "#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=SAMME.R\n",
    "            'random_state': grid_seed\n",
    "            }],       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=gini\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            #'loss': ['deviance', 'exponential'], #default=deviance\n",
    "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=friedman_mse\n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=gini\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{    \n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #default: 100\n",
    "            'random_state': grid_seed\n",
    "            }],        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "             }],            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio, #default: 1.0\n",
    "             }],    \n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
    "            'weights': ['uniform', 'distance'], #default = uniform\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #edfault: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "\n",
    "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
    "    #print(param)    \n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data1[data1_x_bin], data1[Target])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b44d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard Vote or majority rules w/Tuned Hyperparameters\n",
    "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities w/Tuned Hyperparameters\n",
    "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ee401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for modeling\n",
    "print(data_val.info())\n",
    "print(\"-\"*10)\n",
    "#data_val.sample(10)\n",
    "\n",
    "#handmade decision tree - submission score = 0.77990\n",
    "# data_val['Transported'] = mytree(data_val).astype(int)  # 0 V7\n",
    "data_val['Transported'] = mytree(data_val)\n",
    "\n",
    "#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_dt = tree.DecisionTreeClassifier()\n",
    "#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_dt.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n",
    "#submit_bc = ensemble.BaggingClassifier()\n",
    "#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_bc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_etc = ensemble.ExtraTreesClassifier()\n",
    "#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_etc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n",
    "#submit_rfc = ensemble.RandomForestClassifier()\n",
    "#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n",
    "#submit_abc = ensemble.AdaBoostClassifier()\n",
    "#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_abc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n",
    "#submit_gbc = ensemble.GradientBoostingClassifier()\n",
    "#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n",
    "\n",
    "#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n",
    "#submit_xgb = XGBClassifier()\n",
    "#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n",
    "#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#hard voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.74655 V4\n",
    "# data_val['Transported'] = vote_hard.predict(data_val[data1_x_bin])  # 0.74655 V4\n",
    "# data_val['Transported'] = grid_hard.predict(data_val[data1_x_bin])  # 0.70189 V4\n",
    "\n",
    "\n",
    "#soft voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.75005 V6\n",
    "# data_val['Transported'] = vote_soft.predict(data_val[data1_x_bin])  # 0.75005 V6\n",
    "# data_val['Transported'] = grid_soft.predict(data_val[data1_x_bin])  # 0.74982 V5\n",
    "\n",
    "\n",
    "#submit file\n",
    "submit = data_val[['PassengerId','Transported']]\n",
    "submit.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', data_val['Transported'].value_counts(normalize = True))\n",
    "submit.sample(10)\n",
    "\n",
    "# The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 117.91 seconds.\n",
    "# The best parameter for BaggingClassifier is {'max_samples': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 216.62 seconds.\n",
    "# The best parameter for ExtraTreesClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'random_state': 0} with a runtime of 217.29 seconds.\n",
    "# The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300, 'random_state': 0} with a runtime of 336.56 seconds.\n",
    "# The best parameter for RandomForestClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'oob_score': True, 'random_state': 0} with a runtime of 285.97 seconds.\n",
    "# The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 817.48 seconds.\n",
    "# The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'lbfgs'} with a runtime of 338.86 seconds.\n",
    "# The best parameter for BernoulliNB is {'alpha': 0.5} with a runtime of 0.82 seconds.\n",
    "# The best parameter for GaussianNB is {} with a runtime of 0.17 seconds.\n",
    "# The best parameter for KNeighborsClassifier is {'algorithm': 'ball_tree', 'n_neighbors': 7, 'weights': 'distance'} with a runtime of 35.67 seconds.\n",
    "# The best parameter for SVC is {'C': 1, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 3908.73 seconds.\n",
    "# The best parameter for XGBClassifier is {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'seed': 0} with a runtime of 459.98 seconds.\n",
    "# Total optimization time was 112.27 minutes.\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b182a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
